{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fe7f0f",
   "metadata": {},
   "source": [
    "# 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ba1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Loading the different dataframe files\n",
    "data_dir = \"data_final_project/KuaiRec 2.0/data/\"\n",
    "small_df_path = os.path.join(data_dir, \"small_matrix.csv\")\n",
    "big_df_path = os.path.join(data_dir, \"big_matrix.csv\")\n",
    "user_features_path = os.path.join(data_dir, \"user_features.csv\")\n",
    "item_daily_features_path = os.path.join(data_dir, \"item_daily_features.csv\")\n",
    "item_categories_path = os.path.join(data_dir, \"item_categories.csv\")\n",
    "social_network_path = os.path.join(data_dir, \"social_network.csv\")\n",
    "kuairec_caption_category_path = os.path.join(data_dir, \"kuairec_caption_category.csv\")\n",
    "\n",
    "small_df = pd.read_csv(small_df_path)\n",
    "big_df = pd.read_csv(big_df_path)\n",
    "user_features = pd.read_csv(user_features_path)\n",
    "item_daily_features = pd.read_csv(item_daily_features_path)\n",
    "item_categories = pd.read_csv(item_categories_path)\n",
    "social_network = pd.read_csv(social_network_path)\n",
    "kuairec_caption_category = pd.read_csv(kuairec_caption_category_path, quotechar='\"',\n",
    "    escapechar='\\\\',\n",
    "    encoding='utf-8',\n",
    "    engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43aad74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  video_id  play_duration  video_duration                     time  \\\n",
      "0       14       148           4381            6067  2020-07-05 05:27:48.378   \n",
      "1       14       183          11635            6100  2020-07-05 05:28:00.057   \n",
      "2       14      3649          22422           10867  2020-07-05 05:29:09.479   \n",
      "3       14      5262           4479            7908  2020-07-05 05:30:43.285   \n",
      "4       14      8234           4602           11000  2020-07-05 05:35:43.459   \n",
      "\n",
      "         date     timestamp  watch_ratio  \n",
      "0  20200705.0  1.593898e+09     0.722103  \n",
      "1  20200705.0  1.593898e+09     1.907377  \n",
      "2  20200705.0  1.593898e+09     2.063311  \n",
      "3  20200705.0  1.593898e+09     0.566388  \n",
      "4  20200705.0  1.593899e+09     0.418364  \n"
     ]
    }
   ],
   "source": [
    "print(small_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12612464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  video_id  play_duration  video_duration                     time  \\\n",
      "0        0      3649          13838           10867  2020-07-05 00:08:23.438   \n",
      "1        0      9598          13665           10984  2020-07-05 00:13:41.297   \n",
      "2        0      5262            851            7908  2020-07-05 00:16:06.687   \n",
      "3        0      1963            862            9590  2020-07-05 00:20:26.792   \n",
      "4        0      8234            858           11000  2020-07-05 00:43:05.128   \n",
      "\n",
      "       date     timestamp  watch_ratio  \n",
      "0  20200705  1.593879e+09     1.273397  \n",
      "1  20200705  1.593879e+09     1.244082  \n",
      "2  20200705  1.593879e+09     0.107613  \n",
      "3  20200705  1.593880e+09     0.089885  \n",
      "4  20200705  1.593881e+09     0.078000  \n"
     ]
    }
   ],
   "source": [
    "print(big_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373a3a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id user_active_degree  is_lowactive_period  is_live_streamer  \\\n",
      "0        0        high_active                    0                 0   \n",
      "1        1        full_active                    0                 0   \n",
      "2        2        full_active                    0                 0   \n",
      "3        3        full_active                    0                 0   \n",
      "4        4        full_active                    0                 0   \n",
      "\n",
      "   is_video_author  follow_user_num follow_user_num_range  fans_user_num  \\\n",
      "0                0                5                (0,10]              0   \n",
      "1                0              386             (250,500]              4   \n",
      "2                0               27               (10,50]              0   \n",
      "3                0               16               (10,50]              0   \n",
      "4                0              122             (100,150]              4   \n",
      "\n",
      "  fans_user_num_range  friend_user_num  ... onehot_feat8  onehot_feat9  \\\n",
      "0                   0                0  ...          184             6   \n",
      "1              [1,10)                2  ...          186             6   \n",
      "2                   0                0  ...           51             2   \n",
      "3                   0                0  ...          251             3   \n",
      "4              [1,10)                0  ...           99             4   \n",
      "\n",
      "  onehot_feat10  onehot_feat11  onehot_feat12  onehot_feat13  onehot_feat14  \\\n",
      "0             3              0            0.0            0.0            0.0   \n",
      "1             2              0            0.0            0.0            0.0   \n",
      "2             3              0            0.0            0.0            0.0   \n",
      "3             2              0            0.0            0.0            0.0   \n",
      "4             2              0            0.0            0.0            0.0   \n",
      "\n",
      "   onehot_feat15  onehot_feat16  onehot_feat17  \n",
      "0            0.0            0.0            0.0  \n",
      "1            0.0            0.0            0.0  \n",
      "2            0.0            0.0            0.0  \n",
      "3            0.0            0.0            0.0  \n",
      "4            0.0            0.0            0.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(user_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ef6737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['high_active' 'full_active' 'middle_active' 'UNKNOWN']\n"
     ]
    }
   ],
   "source": [
    "# Print the different values of the user_active_degree column\n",
    "print(user_features['user_active_degree'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bbd6000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   video_id      date  author_id video_type   upload_dt  upload_type  \\\n",
      "0         0  20200705       3309     NORMAL  2020-03-30  ShortImport   \n",
      "1         0  20200706       3309     NORMAL  2020-03-30  ShortImport   \n",
      "2         0  20200707       3309     NORMAL  2020-03-30  ShortImport   \n",
      "3         0  20200708       3309     NORMAL  2020-03-30  ShortImport   \n",
      "4         0  20200709       3309     NORMAL  2020-03-30  ShortImport   \n",
      "\n",
      "  visible_status  video_duration  video_width  video_height  ...  \\\n",
      "0         public          5966.0          720          1280  ...   \n",
      "1         public          5966.0          720          1280  ...   \n",
      "2         public          5966.0          720          1280  ...   \n",
      "3         public          5966.0          720          1280  ...   \n",
      "4         public          5966.0          720          1280  ...   \n",
      "\n",
      "   download_cnt  download_user_num report_cnt  report_user_num  \\\n",
      "0             8                  8          0                0   \n",
      "1             2                  2          0                0   \n",
      "2             2                  2          0                0   \n",
      "3             3                  3          0                0   \n",
      "4             2                  2          2                1   \n",
      "\n",
      "   reduce_similar_cnt  reduce_similar_user_num  collect_cnt  collect_user_num  \\\n",
      "0                   3                        3          NaN               NaN   \n",
      "1                   5                        5          NaN               NaN   \n",
      "2                   0                        0          NaN               NaN   \n",
      "3                   3                        3          NaN               NaN   \n",
      "4                   1                        1          NaN               NaN   \n",
      "\n",
      "   cancel_collect_cnt  cancel_collect_user_num  \n",
      "0                 NaN                      NaN  \n",
      "1                 NaN                      NaN  \n",
      "2                 NaN                      NaN  \n",
      "3                 NaN                      NaN  \n",
      "4                 NaN                      NaN  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(item_daily_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba2451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   video_id     feat\n",
      "0         0      [8]\n",
      "1         1  [27, 9]\n",
      "2         2      [9]\n",
      "3         3     [26]\n",
      "4         4      [5]\n"
     ]
    }
   ],
   "source": [
    "print(item_categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c52e647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id friend_list\n",
      "0     3371      [2975]\n",
      "1       24      [2665]\n",
      "2     4402        [38]\n",
      "3     4295      [4694]\n",
      "4     7087      [7117]\n"
     ]
    }
   ],
   "source": [
    "print(social_network.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c107dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  video_id manual_cover_text  \\\n",
      "0        0           UNKNOWN   \n",
      "1        1           UNKNOWN   \n",
      "2        2           UNKNOWN   \n",
      "3        3           UNKNOWN   \n",
      "4        4      五爱街最美美女 一天1q   \n",
      "\n",
      "                                             caption            topic_tag  \\\n",
      "0                                   精神小伙路难走 程哥你狗粮慢点撒                   []   \n",
      "1                                                NaN                   []   \n",
      "2                                          晚饭后，运动一下！                   []   \n",
      "3  我平淡无奇，惊艳不了时光，温柔不了岁月，我只想漫无目的的走走，努力发笔小财，给自己买花 自己长大.                   []   \n",
      "4                     #搞笑 #感谢快手我要上热门 #五爱市场 这真是完美搭配啊！  [五爱市场,感谢快手我要上热门,搞笑]   \n",
      "\n",
      "   first_level_category_id first_level_category_name  \\\n",
      "0                      8.0                        颜值   \n",
      "1                     27.0                      高新数码   \n",
      "2                      9.0                        喜剧   \n",
      "3                     26.0                        摄影   \n",
      "4                      5.0                        时尚   \n",
      "\n",
      "   second_level_category_id second_level_category_name  \\\n",
      "0                     673.0                       颜值随拍   \n",
      "1                    -124.0                    UNKNOWN   \n",
      "2                     727.0                       搞笑互动   \n",
      "3                     686.0                       主题摄影   \n",
      "4                     737.0                       营销售卖   \n",
      "\n",
      "   third_level_category_id third_level_category_name  \n",
      "0                   -124.0                   UNKNOWN  \n",
      "1                   -124.0                   UNKNOWN  \n",
      "2                   -124.0                   UNKNOWN  \n",
      "3                   2434.0                      景物摄影  \n",
      "4                   2596.0                        女装  \n"
     ]
    }
   ],
   "source": [
    "print(kuairec_caption_category.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a4dc7",
   "metadata": {},
   "source": [
    "**Conclusion**: In the context of this project, the dataframes that will be used are big_df, small_df and item_features_daily. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069ee8c",
   "metadata": {},
   "source": [
    "# 2. Data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f5792a",
   "metadata": {},
   "source": [
    "Cells inspecting Nan values in user-item interaction matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70da837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small DataFrame shape: (4676570, 8)\n",
      "Big DataFrame shape: (12530806, 8)\n"
     ]
    }
   ],
   "source": [
    "# Printing the shapes of the interaction dataframes\n",
    "print(\"Small DataFrame shape:\", small_df.shape)\n",
    "print(\"Big DataFrame shape:\", big_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b29b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN user IDs in small_df: 0\n",
      "Number of NaN user IDs in big_df: 0\n",
      "Number of NaN video IDs in small_df: 0\n",
      "Number of NaN video IDs in big_df: 0\n",
      "Number of NaN video IDs in big_df: 0\n"
     ]
    }
   ],
   "source": [
    "# Checking for NaN values in user_id and video_id columns\n",
    "small_nan_user = small_df[small_df['user_id'].isna()]\n",
    "print(\"Number of NaN user IDs in small_df:\", len(small_nan_user))\n",
    "big_nan_user = big_df[big_df['user_id'].isna()]\n",
    "print(\"Number of NaN user IDs in big_df:\", len(big_nan_user))\n",
    "\n",
    "small_nan_video = small_df[small_df['video_id'].isna()]\n",
    "print(\"Number of NaN video IDs in small_df:\", len(small_nan_video))\n",
    "big_nan_video = big_df[big_df['video_id'].isna()]\n",
    "print(\"Number of NaN video IDs in big_df:\", len(big_nan_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96184829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in small_df: 181992\n",
      "Number of NaN values in big_df: 0\n",
      "Number of NaN values in big_df: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the rest of the columns for NaN values\n",
    "small_nan_columns = small_df[small_df.isna().any(axis=1)]\n",
    "print(\"Number of NaN values in small_df:\", len(small_nan_columns))\n",
    "big_nan_columns = big_df[big_df.isna().any(axis=1)]\n",
    "print(\"Number of NaN values in big_df:\", len(big_nan_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09d56679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values in small_df: ['time', 'date', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "# Find the columns in small_df that have NaN values\n",
    "small_nan_columns = small_df.columns[small_df.isna().any()].tolist()\n",
    "print(\"Columns with NaN values in small_df:\", small_nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b79d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows in small_df: 0\n",
      "Number of duplicate rows in big_df: 965819\n",
      "Number of duplicate rows in big_df: 965819\n"
     ]
    }
   ],
   "source": [
    "# Check small_df and big_df for duplicates\n",
    "small_duplicates = small_df[small_df.duplicated()]\n",
    "print(\"Number of duplicate rows in small_df:\", len(small_duplicates))\n",
    "big_duplicates = big_df[big_df.duplicated()]\n",
    "print(\"Number of duplicate rows in big_df:\", len(big_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8d2233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in big_df after removal: 11564987\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates from big_df\n",
    "big_df = big_df.drop_duplicates()\n",
    "print(\"Number of rows in big_df after removal:\", len(big_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5f412",
   "metadata": {},
   "source": [
    "The big matrix has no Nan values, and the small matrix only has some NaN values in the columns ['time', 'date', 'timestamp']. The fact that only the small matrix, which is the one we use for the evaluation of the model, has NaN values simulates real situation where we can lack information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e42b5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in small_df: 1411\n",
      "Number of unique users in big_df: 7176\n",
      "Number of unique users in big_df: 7176\n",
      "Number of unique items in small_df: 3327\n",
      "Number of unique items in small_df: 3327\n",
      "Number of unique items in big_df: 10728\n",
      "Number of unique items in big_df: 10728\n"
     ]
    }
   ],
   "source": [
    "# Check the number of users after removing duplicates\n",
    "print(\"Number of unique users in small_df:\", small_df['user_id'].nunique())\n",
    "print(\"Number of unique users in big_df:\", big_df['user_id'].nunique())\n",
    "\n",
    "print(\"Number of unique items in small_df:\", small_df['video_id'].nunique())\n",
    "print(\"Number of unique items in big_df:\", big_df['video_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a34c75",
   "metadata": {},
   "source": [
    "Looking at the other matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df8a2fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All user IDs in small_df are present in user_features.\n",
      "All user IDs in big_df are present in user_features.\n",
      "Missing user IDs from small_df in big_df: Empty DataFrame\n",
      "Columns: [user_id, video_id, play_duration, video_duration, time, date, timestamp, watch_ratio]\n",
      "Index: []\n",
      "Missing user IDs from small_df in big_df: Empty DataFrame\n",
      "Columns: [user_id, video_id, play_duration, video_duration, time, date, timestamp, watch_ratio]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check if all user IDs in small_df are present in user_features\n",
    "missing_users = set(small_df['user_id']) - set(user_features['user_id'])\n",
    "if missing_users:\n",
    "    print(\"Missing user IDs in user_features:\", missing_users)\n",
    "else:\n",
    "    print(\"All user IDs in small_df are present in user_features.\")\n",
    "# Check if all user IDs in big_df are present in user_features\n",
    "missing_users = set(big_df['user_id']) - set(user_features['user_id'])\n",
    "if missing_users:\n",
    "    print(\"Missing user IDs in user_features:\", missing_users)\n",
    "else:\n",
    "    print(\"All user IDs in big_df are present in user_features.\")\n",
    "\n",
    "missing_users = big_df[~big_df['user_id'].isin(user_features['user_id'])]\n",
    "print(\"Missing user IDs from small_df in big_df:\", missing_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2723aac",
   "metadata": {},
   "source": [
    "There are no missing users from the user matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119ed3f",
   "metadata": {},
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d32e91",
   "metadata": {},
   "source": [
    "## Analysis: Percentage of Videos Watched >50%  and >90% in small_df\n",
    "Check what proportion of user-video interactions in small_df are highly engaged to choose with threshold to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99b7a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of videos in small_df watched more than 50%: 72.73%\n"
     ]
    }
   ],
   "source": [
    "relevant_videos = small_df['watch_ratio'] > 0.5\n",
    "percentage_relevant = relevant_videos.mean() * 100\n",
    "print(f\"Percentage of videos in small_df watched more than 50%: {percentage_relevant:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de71ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of videos in small_df watched approximately the full video: 39.60%\n"
     ]
    }
   ],
   "source": [
    "relevant_videos = small_df['watch_ratio'] > 0.9\n",
    "percentage_relevant = relevant_videos.mean() * 100\n",
    "print(f\"Percentage of videos in small_df watched approximately the full video: {percentage_relevant:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea20b16",
   "metadata": {},
   "source": [
    "**Conclusion** The 90% threshold is better as the videos that fulfill the condition are rare enough.\n",
    "*Note* 90% is used instead of 100% for the comparison, to consider the videos that don't have relevant content in the last 10% as fully watched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50730d",
   "metadata": {},
   "source": [
    "## Data Preparation for Hybrid Recommender\n",
    "Prepare and save the data required for ALS (collaborative filtering) and LogisticRegression (content-based filtering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46293c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS interaction matrix saved.\n"
     ]
    }
   ],
   "source": [
    "# --- ALS Data Preparation ---\n",
    "# Create user-item interaction matrix (watch_ratio as implicit feedback)\n",
    "interaction_matrix = big_df.pivot_table(index='user_id', columns='video_id', values='watch_ratio', fill_value=0)\n",
    "interaction_matrix.to_csv('als_train_matrix.csv')\n",
    "print('ALS interaction matrix saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cdd7d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-based features and labels saved (90% threshold, new features).\n"
     ]
    }
   ],
   "source": [
    "# --- Content-Based Data Preparation ---\n",
    "# Merge big_df with item_daily_features and item_categories for video features\n",
    "item_features = pd.merge(big_df[['video_id']].drop_duplicates(),\n",
    "                        item_daily_features.drop_duplicates('video_id'),\n",
    "                        on='video_id', how='left')\n",
    "item_features = pd.merge(item_features,\n",
    "                        item_categories.drop_duplicates('video_id'),\n",
    "                        on='video_id', how='left')\n",
    "# Use meaningful features for content-based filtering\n",
    "feature_cols = [\n",
    "    'download_cnt', 'report_cnt', 'show_cnt', 'play_cnt', 'like_cnt',\n",
    "    'comment_cnt', 'share_cnt', 'follow_cnt', 'video_duration'\n",
    "]\n",
    "X_content = item_features[feature_cols].fillna(0)\n",
    "# For y_content, use whether user watched >90% of video as positive class\n",
    "big_df['watched_majority'] = big_df['watch_ratio'] > 0.9\n",
    "y_content = big_df.groupby('video_id')['watched_majority'].mean().round().astype(int).reindex(item_features['video_id']).fillna(0)\n",
    "X_content.to_csv('content_features_X.csv')\n",
    "y_content.to_frame('watched_majority').to_csv('content_features_y.csv')\n",
    "print('Content-based features and labels saved (90% threshold, new features).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5044a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small DataFrame with relevance saved.\n"
     ]
    }
   ],
   "source": [
    "small_df['relevant'] = small_df['watch_ratio'] > 0.9\n",
    "small_df.to_csv('small_matrix_with_relevance.csv', index=False)\n",
    "print('Small DataFrame with relevance saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e73536c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation data saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation Data Preparation ---\n",
    "# For demonstration, use small_df for evaluation\n",
    "# Prepare user-item pairs and content features for evaluation\n",
    "small_pairs = small_df[['user_id', 'video_id']]\n",
    "small_pairs.to_csv('eval_pairs.csv', index=False)\n",
    "# Get content features for videos in small_df\n",
    "eval_item_features = item_features[item_features['video_id'].isin(small_df['video_id'])]\n",
    "X_eval_content = eval_item_features[feature_cols].fillna(0)\n",
    "X_eval_content.to_csv('eval_content_features_X.csv')\n",
    "print('Evaluation data saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21089d5",
   "metadata": {},
   "source": [
    "# 4. Baselines establishement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1cc197",
   "metadata": {},
   "source": [
    "## Baseline 1: Top-N Most Watched Videos for All Users\n",
    "This baseline recommends the globally most-watched videos to every user. Good for cold start, but poor for discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e06f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 most watched videos: [8145, 9485, 1037, 8716, 4500, 6938, 9059, 9758, 430, 10174, 8769, 3827, 1182, 481, 10069, 1164, 4268, 951, 7508, 7601, 1175, 5448, 7535, 8413, 6948, 7031, 7561, 2871, 5066, 3876, 9790, 3739, 8983, 2765, 5944, 10067, 4437, 7493, 945, 5964, 5853, 7014, 4365, 300, 4506, 1988, 883, 5274, 2632, 2756, 10044, 8446, 848, 7318, 6854, 2113, 8770, 7362, 297, 8137, 1095, 9683, 9022, 4599, 10084, 265, 3699, 183, 4605, 1507, 8773, 7291, 4266, 1161, 8848, 429, 6974, 10235, 3778, 4321, 2932, 394, 3647, 9667, 9792, 2745, 9711, 4404, 6829, 4335, 2212, 8937, 3185, 5383, 4819, 8915, 4363, 7574, 10014, 10238]\n",
      "Recommendations for user 14: [8145, 9485, 1037, 8716, 4500, 6938, 9059, 9758, 430, 10174, 8769, 3827, 1182, 481, 10069, 1164, 4268, 951, 7508, 7601, 1175, 5448, 7535, 8413, 6948, 7031, 7561, 2871, 5066, 3876, 9790, 3739, 8983, 2765, 5944, 10067, 4437, 7493, 945, 5964, 5853, 7014, 4365, 300, 4506, 1988, 883, 5274, 2632, 2756, 10044, 8446, 848, 7318, 6854, 2113, 8770, 7362, 297, 8137, 1095, 9683, 9022, 4599, 10084, 265, 3699, 183, 4605, 1507, 8773, 7291, 4266, 1161, 8848, 429, 6974, 10235, 3778, 4321, 2932, 394, 3647, 9667, 9792, 2745, 9711, 4404, 6829, 4335, 2212, 8937, 3185, 5383, 4819, 8915, 4363, 7574, 10014, 10238]\n"
     ]
    }
   ],
   "source": [
    "# Compute top-N most watched videos (by total watch count in big_df)\n",
    "top_n = 100  # Change as needed\n",
    "top_videos = big_df.groupby('video_id')['watch_ratio'].count().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "print(f\"Top-{top_n} most watched videos: {top_videos}\")\n",
    "# Example: recommend to a user\n",
    "example_user = small_df['user_id'].iloc[0]\n",
    "print(f\"Recommendations for user {example_user}: {top_videos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41aa9e6",
   "metadata": {},
   "source": [
    "## Baseline 2: Top-N Most Watched Videos by First Friend\n",
    "This baseline recommends the top-N most watched videos by the first friend in the user's friend list. If no friends, fallback to global top-N. Good for discovery, but not for cold start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fbebde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 14 by first friend: [8145, 9485, 1037, 8716, 4500, 6938, 9059, 9758, 430, 10174, 8769, 3827, 1182, 481, 10069, 1164, 4268, 951, 7508, 7601, 1175, 5448, 7535, 8413, 6948, 7031, 7561, 2871, 5066, 3876, 9790, 3739, 8983, 2765, 5944, 10067, 4437, 7493, 945, 5964, 5853, 7014, 4365, 300, 4506, 1988, 883, 5274, 2632, 2756, 10044, 8446, 848, 7318, 6854, 2113, 8770, 7362, 297, 8137, 1095, 9683, 9022, 4599, 10084, 265, 3699, 183, 4605, 1507, 8773, 7291, 4266, 1161, 8848, 429, 6974, 10235, 3778, 4321, 2932, 394, 3647, 9667, 9792, 2745, 9711, 4404, 6829, 4335, 2212, 8937, 3185, 5383, 4819, 8915, 4363, 7574, 10014, 10238]\n"
     ]
    }
   ],
   "source": [
    "# get social network data\n",
    "def get_first_friend(user_id):\n",
    "    row = social_network[social_network['user_id'] == user_id]\n",
    "    if row.empty or pd.isna(row.iloc[0]['friend_list']):\n",
    "        return None\n",
    "    # Friend_list is a string of comma-separated user_ids, with brackets\n",
    "    friends = str(row.iloc[0]['friend_list']).replace('[', '').replace(']', '').split(',')\n",
    "    friends = [f.strip() for f in friends if f.strip()]\n",
    "    return friends[0] if friends else None\n",
    "\n",
    "def recommend_by_first_friend(user_id, n=100):\n",
    "    friend_id = get_first_friend(user_id)\n",
    "    if friend_id is not None:\n",
    "        friend_videos = big_df[big_df['user_id'] == int(friend_id)]\n",
    "        top_friend_videos = friend_videos.groupby('video_id')['watch_ratio'].count().sort_values(ascending=False).head(n).index.tolist()\n",
    "        if top_friend_videos:\n",
    "            return top_friend_videos\n",
    "    # Fallback to global top-N\n",
    "    return top_videos\n",
    "\n",
    "# Example usage\n",
    "print(f\"Recommendations for user {example_user} by first friend: {recommend_by_first_friend(example_user, top_n)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7473e568",
   "metadata": {},
   "source": [
    "## Evaluate the baselines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a4289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 1 (Top-100 Most Watched) results saved to baseline1_results.csv\n",
      "Mean Precision: 0.3592\n",
      "Mean Recall: 0.0272\n",
      "Mean F1: 0.0503\n",
      "Mean NDCG@100: 0.3345\n",
      "Baseline 2 (Top-100 by First Friend) results saved to baseline2_results.csv\n",
      "Mean Precision: 0.3423\n",
      "Mean Recall: 0.0259\n",
      "Mean F1: 0.0480\n",
      "Mean NDCG@100: 0.3193\n",
      "Baseline 2 (Top-100 by First Friend) results saved to baseline2_results.csv\n",
      "Mean Precision: 0.3423\n",
      "Mean Recall: 0.0259\n",
      "Mean F1: 0.0480\n",
      "Mean NDCG@100: 0.3193\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline 1 for all users in small_df\n",
    "N = 100\n",
    "user_ids = small_df['user_id'].unique()\n",
    "def ndcg_at_k(recommended_items, ground_truth_items, k):\n",
    "    def dcg(recs, gt):\n",
    "        return sum([1 / np.log2(i + 2) if rec in gt else 0 for i, rec in enumerate(recs[:k])])\n",
    "    ndcgs = []\n",
    "    for user in user_ids:\n",
    "        rec_items = recommended_items(user)\n",
    "        true_items = set(small_df[(small_df['user_id'] == user) & (small_df['relevant'])]['video_id'])\n",
    "        if not true_items:\n",
    "            continue\n",
    "        dcg_val = dcg(rec_items, true_items)\n",
    "        idcg_val = sum([1 / np.log2(i + 2) for i in range(min(len(true_items), k))])\n",
    "        ndcgs.append(dcg_val / idcg_val if idcg_val > 0 else 0)\n",
    "    return np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "# Baseline 1: Top-N Most Watched Videos for All Users\n",
    "baseline1_results = []\n",
    "for user in user_ids:\n",
    "    gt = set(small_df[(small_df['user_id'] == user) & (small_df['relevant'])]['video_id'])\n",
    "    hits = set(top_videos) & gt\n",
    "    precision = len(hits) / N if N > 0 else 0\n",
    "    recall = len(hits) / len(gt) if len(gt) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    baseline1_results.append({'user_id': user, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "def baseline1_recs(user):\n",
    "    return top_videos\n",
    "ndcg1 = ndcg_at_k(baseline1_recs, small_df, N)\n",
    "pd.DataFrame(baseline1_results).to_csv('baseline1_results.csv', index=False)\n",
    "print(f'Baseline 1 (Top-{N} Most Watched) results saved to baseline1_results.csv')\n",
    "print(f\"Mean Precision: {np.mean([r['precision'] for r in baseline1_results]):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean([r['recall'] for r in baseline1_results]):.4f}\")\n",
    "print(f\"Mean F1: {np.mean([r['f1'] for r in baseline1_results]):.4f}\")\n",
    "print(f\"Mean NDCG@{N}: {ndcg1:.4f}\")\n",
    "\n",
    "# Baseline 2: Top-N Most Watched Videos by First Friend\n",
    "baseline2_results = []\n",
    "def baseline2_recs(user):\n",
    "    return recommend_by_first_friend(user, N)\n",
    "for user in user_ids:\n",
    "    recs = recommend_by_first_friend(user, N)\n",
    "    gt = set(small_df[(small_df['user_id'] == user) & (small_df['relevant'])]['video_id'])\n",
    "    hits = set(recs) & gt\n",
    "    precision = len(hits) / N if N > 0 else 0\n",
    "    recall = len(hits) / len(gt) if len(gt) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    baseline2_results.append({'user_id': user, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "ndcg2 = ndcg_at_k(baseline2_recs, small_df, N)\n",
    "pd.DataFrame(baseline2_results).to_csv('baseline2_results.csv', index=False)\n",
    "print(f'Baseline 2 (Top-{N} by First Friend) results saved to baseline2_results.csv')\n",
    "print(f\"Mean Precision: {np.mean([r['precision'] for r in baseline2_results]):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean([r['recall'] for r in baseline2_results]):.4f}\")\n",
    "print(f\"Mean F1: {np.mean([r['f1'] for r in baseline2_results]):.4f}\")\n",
    "print(f\"Mean NDCG@{N}: {ndcg2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".recom_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
